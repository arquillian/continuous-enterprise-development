== Enabling Technologies

_"I get by with a little help from my friends." - Paul McCartney and John Lennon_

There's a common misconception that the goal of a standard specification is to address _every_ problem.  This couldn't be further from the truth; creating a standard is meant to address the 80% case in a manner that's been proven through experience in the field.  The http://www.oracle.com/technetwork/java/javaee/downloads/index.html[Java Enterprise Edition] and its subsystems, governed by the http://www.jcp.org/en/home/index[Java Community Process]  (JCP), is no exception.  

By its very makeup, the JCP is designed to strive for consensus among all participants in an Expert Group on a given technology.  Where corporate sponsors and individual contributors disagree or determine that a feature is not yet mature enough to be adequately standardized, latitude is given to specification implementors.  This helps to foster creativity and provides differentiation between vendors.  In fact, on a http://java.net/projects/javaee-spec/lists/jsr342-experts/archive/2012-08/message/16[discussion regarding the Java EE7 Roadmap], Expert Group member David Blevins succinctly addressed the dynamic: "Vendors innovate, collectively we standardize."

While it's not the goal of this book to provide exhaustive instruction on the complete featureset of Java EE, it is absolutely our intent to unify the development experience.  Helping us along the way are a set of enabling technologies intending to smooth the rough edges of the EE Platform and fill the gaps left open by its specifications.

The following open-source projects are all made freely-available for you to download, use, and modify (be sure to consult individual licensing terms).

=== Bootstrapping

For all the documentation surrounding Java EE and its use, the seemingly simple act of getting started gets quickly muddled:

* How am I going to build my sources into deployments?
* How should I organize my codebase?
* How can my team best collaborate in parallel on the codebase?
* What about libraries my code uses?  How do I get those?

There are a number of valid answers to each of these questions, and the flexibility of choice can easily turn into a burden.  Because we'll be exploring fully-functioning examples which are intended to be reproduced in your own environment, by necessity we've had to make some decisions in the interest of keeping focus on the code as opposed to our development tools.  The projects below, when combined, work very well together but are certainly not the only solutions to the bullets raised above.

One approach to undertaking a new project is to first lay out the scaffolding on your local file system.  This will create the structure for your source code, build descriptors, and other resources used by your project.  Often this process is fairly rote, involving commands to make new directories and text files in some sensible layout.  While there's no formal rule dictating how your project tree is organized, some build systems employ a convention; others instead choose to allow you total control over your project's build by encouraging you to script or otherwise instruct each build task.

Our examples will be built using a _declarative build tool_ which has standard commands that do not change from project to project.

==== Apache Maven

image:images/ch02-enabling_technologies/maven.png["Apache Maven"]

Perhaps the most prominent figure in the Java automated build tool landscape, http://maven.apache.org/[Apache Maven] positions itself as a "software project management and comprehension tool".  For simplicity's sake, we may view it as a build tool; it's capable of compiling, testing, and assembling.  

One very nice feature of Maven is that it strives for _"convention over configuration"_.  By following a set of recommended best practices, you're likely to trim down on the amount of metadata you'd otherwise need to explicitly define.  Additionally, Maven actions (called _goals_) are bound to a documented http://maven.apache.org/guides/introduction/introduction-to-the-lifecycle.html[lifecycle] which are common to all Maven-based projects.  For instance, in order to compile, test, and package your project, the command +$> mvn package+ applies.  This standardization alleviates us from having to declare or learn different build commands for each project.

At the core of the Maven engine is a sophisticated _dependency management_ solution capable of resolving libraries by name from a http://search.maven.org/[Central Repository] (or additionally-configured repository) onto a user's local system.  This feature allows us to skip the manual process of adding dependencies into our version control system, and allows us to instead fetch them on-demand as part of the build process.  As an added bonus, the requisite dependencies for all projects consuming ours are well-documented and automatically fetched for us.

***INSERT IMAGE : Project A depends on B depends on C, gets all as requested from the Central Repo***

Maven is not without its detractors, however.  It's been criticized for a few points, among them:

* Maven Plugins versions are not bound to Maven Core versions, making guaranteed reproducible builds between different environments difficult to guarantee.
* Project Object Model (POM, ie. +pom.xml+) syntax, the metadata describing a project's makeup, is verbose.
* Transitive dependencies as a default trigger a lot of downloading on first build.  Without care, a project may inherit more dependencies than are necessary or desired.
* Deviation from the defined Maven standard is often difficult to reconcile.

It _is_ possible to use Maven-structured repositories from outside Maven.  In fact, standalone dependency manager http://ant.apache.org/ivy/[Apache Ivy] (often used in concert with task-based tool http://ant.apache.org/[Apache Ant]), does just that.  Groovy-based http://www.gradle.org/[Gradle] seeks to provide the flexibility of Ant with the dependency management of Maven.

That said, Maven continues to be a popular and widely-used tool in Java development, and will satisfy our requirements to build our examples.

==== JBoss Forge

image:images/ch02-enabling_technologies/forge.png["JBoss Forge"]

If you've spent any time developing Java EE-based projects (or any nontrivial application, for that matter!), you've likely invested a good amount of energy in creating the project layout, defining dependencies, and informing the build system of the relevant ClassPaths to be used in compilation and execution.  While Maven enables us to reduce that load as compared with undertaking project setup manually, there's typically quite a bit of boilerplate involved in the +pom.xml+ defining your requirements.

http://forge.jboss.org/[JBoss Forge] offers "incremental project enhancement for Java EE".  Implemented as a command shell, Forge gives us the ability to alter project files and folders.  Some concrete tasks we might use Forge to handle are:

* Adding _Java Persistence API_ (JPA) entities and describing their model
* Configuring Maven dependencies
* Setting up project scaffolding
* Generating a view layer, reversed-engineered from a domain model
* Deploying to an application server

Because Forge is built atop a _modular, http://forge.jboss.org/plugins.html[plugin]-based architecture_, it's extensible to additional tasks that may be specific to your application.

Overall, the goal of Forge is to ease project setup at all stages of development, so we'll be employing it in this text to speed along the construction of our examples.

=== Version Control

From the moment we collaborate on a project with others or would like to inspect the evolution of our code over time, we need some form of _version control_.  Until recently, the most common paradigm for synchronizing access to a shared codebase was the _client/server_ model wherein developers may keep a local working copy and check their changes into a centralized server.  

***INSERT IMAGE SHOWING CENTRALIZED VERSION CONTROL***

Some systems utilize file-level locking to ensure that no conflicts arise during development; others allow concurrent access at the file granularity but cue the developer to resolve line-level conflicts upon committing changes upstream.

Likely the widest-deployed client/server version control system (VCS) from the 1990s through the 2000s has been http://savannah.nongnu.org/projects/cvs[Concurrent Versions Systems], most often referred by its acronym _CVS_.  While CVS has enabled teams to freely work on all files in the tree through _unreserved checkouts_, its shortcomings including non-atomic commits and absent tracking for file renames prompted the development of http://subversion.apache.org/[Subversion] (SVN), hier apparent to CVS.  Boasting a wider featureset and greater stability as contrasted with CVS, SVN has enjoyed its reign from the mid- to late-2000s.

These days, the centralized model has been superseded by _distributed version control systems_ (DVCS), which is differentiated by its ability to store the full repository including all history in any number of nodes.

***INSERT IMAGE SHOWING DVCS***

This layout creates a "pull model", where developers on a common project are given the authority over their own repository, free to incorporate changes from others (or not!).  At first, this can be a confusing topic to grasp for users vested in the centralized "push model", but its our opinion that the benefits of this design easily justify the initial confusion inherent when considering many full-fledged repositories representing the same project.

Some immediate gains to consider:

* Repository operations such as committing and searching history are much faster
* Network connectivity is not required to alter the respository's state
* Every repository is a full backup of the codebase's history

This is because each user is typically working on a local repository, and synchronization with a remote repository is only necessary when pushing changes to be visible by others.

In this text, we'll be using the open-source DVCS _Git_. 

==== Git

image:images/ch02-enabling_technologies/git.png["Git"]

Originally developed to coordinate development of the Linux Kernel, Git is a DVCS whose usage has taken off in recent years, arguably due to the user-friendliness of the socially-aware hosting site http://www.github.com[GitHub].  In fact, this book's text and examples are https://github.com/arquillian/continuous-enterprise-development[hosted] on GitHub for all to participate.

From a high-level, we've chosen Git for our projects as it enables:

* True feature (topic) development.  Branching is quick, easy, and cheap.  You may work on feature X in isolation with the ability to put your changes _on top of_ development that may be occurring in the mainline branch.
* Integration with 3rd-party systems built to respond to Git events.  For instance, we'll be able to trigger builds and production deployments by pushing our local changes to a remote repository.
* Rewriting of local history.  Often it's handy to commit liberally, giving yourself many "save" points along the way.  However, before making these (sometimes breaking) changes visible to the rest of the world, it's good practice to "squash" the mini-changes into a cohesive, singular commit.  This helps keep the version history sane and facilitates later auditing if a bug should arise.

Again, it is not our aim to fully delve into the mechanics of each tool we'll be employing.  However, we will be issuing Git commands and explaining their use along the way.  A very good reference on the myriad Git subroutines can be found in the http://git-scm.com/book[Pro Git Book] by Scott Chacon, available for free in digital editions and in print via online retailers.

=== A Test Platform for Java EE

Java EE 5 introduced a _POJO_ (Plain Old Java Object) programming model which freed developers from having to adhere to any particular class hierarchy for its business objects.  The introduction of http://jcp.org/en/jsr/detail?id=299[Contexts and Dependency Injection] (CDI) in Java EE 6 further pushed the notion of simple business objects by providing _typesafe injection_.  

The benefit to objects that can be easily created using the +new+ operator is the same as their drawback; when we manually instantiate objects for use in testing, we're not dealing with the same enterprise components we have in the target runtime.  An EJB becomes such only in the context of an EJB container; a Servlet is a Servlet only when created by a Servlet Container.  Any time we circumvent the target runtime environment to handle object creation and wiring on our own, we're using _mock objects_.

While many will advocate on the usefulness of mocks, by definition they provide an approximation of how your application will behave in a production environment.  Remember that you're responsible for validating that the full bevy of code running on your servers is working as expected, including the bits you _did not write_.  There are many not-so-subtle errors that may arise while leveraging the full potential of the application server in production, and it's best to be testing in an environment as close to the real thing as possible.

True Java EE testing in this sense is an area left largely unspecified by the EE Platform, and we'll be examining some tools to help bridge this divide.

==== Arquillian

image:images/ch02-enabling_technologies/arquillian.png["Arquillian"]

http://arquillian.org[Arquillian] is an innovative and highly extensible testing platform for the JVM that enables developers to easily create automated integration, functional and acceptance tests for Java middleware.

Picking up where unit tests leave off, Arquillian handles all the plumbing of container management, deployment and framework initialization so you can focus on the business of writing test logic.  Instead of configuring a potentially-complex test harness, Arquillian abstracts out the target runtime by:

* Managing the lifecycle of the container (or containers)
* Bundling the test case, dependent classes and resources into a ShrinkWrap archive (or archives)
* Deploying the archive (or archives) to the container (or containers)
* Enriching the test case by providing dependency injection and other declarative services
* Executing the tests inside (or against) the container
* Capturing the results and returning them to the test runner for reporting
* To avoid introducing unnecessary complexity into the developer’s build environment, Arquillian integrates seamlessly with familiar testing frameworks (e.g., JUnit 4, TestNG 5), allowing tests to be launched using existing IDE, Ant and Maven test plugins — without any add-ons.

The Arquillian project adheres to three core principles:

* *Tests should be portable to any supported container*.  Keeping container-specific APIs out of the tests enables developers to verify application portability by running tests in a variety of containers. It also means that lightweight containers can be used as a substitute for full containers during development.
* *Tests should be executable from both the IDE and the build tool*.  By leveraging the IDE, the developer can skip the build for a faster turnaround and has a familiar environment for debugging. These benefits shouldn’t sacrifice the ability to run the tests in continuous integration using a build tool.
* *The platform should extend or integrate existing test frameworks*.  An extensible architecture encourages reuse of existing software and fosters a unified Java testing ecosystem.  Regardless of how complex it becomes, executing an Arquillian test is as simple as selecting “Run As > Test” in the IDE or executing the “test” goal from the build tool.

***NOTE: Get a better image for this***
image:images/ch02-enabling_technologies/shrinkwrap_runas_junit.png["RunAs > JUnit"]

==== ShrinkWrap

image:images/ch02-enabling_technologies/shrinkwrap.png["ShrinkWrap"]

From the onset, ShrinkWrap was born from a need to more easily test Java Enterprise deployments. Traditionally defined as flat-file archives adhering to the ZIP standard, these have necessitated the introduction of some build step to package up all application resources. And a build step takes time:

----
$ mvn clean install
... terrifying output trace ...
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 1:13.492s
[INFO] ------------------------------------------------------------------------
----

But as developers, we live in our coding environments. Switching out of that mindset to run a build is wasteful.  So we asked: "What if we could declare, in Java, an object to represent that archive?"  What resulted was a Java API analogue to the "jar" tool, a virtual filesystem with an intuitive syntax.

[source,java]
----
JavaArchive archive = ShrinkWrap.create(JavaArchive.class,"myarchive.jar") 
   .addClasses(MyClass.class, MyOtherClass.class)
   .addResource("mystuff.properties");
----

This enables us to take advantage of the IDE’s incremental compilation features, allowing us to skip the build.

*** NOTE: Get a better image for this
image:images/ch02-enabling_technologies/shrinkwrap_incremental_compilation.png["ShrinkWrap Incremental Compilation"]

This piece fulfills the design goal of Arquillian to run tests based on full-fledged deployments directly from the IDE.

While ShrinkWrap is a standalone virtual filesystem, in our examples we'll be primarily exercising it as the deployment mechanism for Arquillian.  Let's take a moment to review its usage.

The first step is getting your hands on the ShrinkWrap binaries. The Core is composed of three pieces:

|=========================
|_Name_|_Maven Coordinates_
|API|org.jboss.shrinkwrap:shrinkwrap-api
|SPI|org.jboss.shrinkwrap:shrinkwrap-spi
|Implementation|org.jboss.shrinkwrap:shrinkwrap-impl-base
|=========================

Only the API should be available upon your compilation ClassPath, while the SPI and the Implementation modules are both required for the runtime. This is to enforce good separation between classes intended for direct use and the project’s internals.

In Maven, these may be brought in under the proper scopes easily by using the ShrinkWrap Dependency Chain POM, available in Maven Central:

[source,xml]
----
<project xmlns="http://maven.apache.org/POM/4.0.0"
  xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
  xsi:schemaLocation="
  http://maven.apache.org/POM/4.0.0
  http://maven.apache.org/xsd/maven-4.0.0.xsd">
  <!-- snip -->
  
  <dependency>
    <groupId>org.jboss.shrinkwrap</groupId>
    <artifactId>shrinkwrap-depchain</artifactId>
    <version>${version.shrinkwrap}</version>
    <type>pom</type>
  </dependency>

  <!-- snip -->
</project>
----

For projects outside use of the Maven repository system, the ShrinkWrap Distribution makes all modules available as a download, and you may set up the dependencies manually to suit your needs.

_Prerequisites_

* JRE5+ Runtime
* No additional dependencies

ShrinkWrap may run on any Java5 runtime or higher, but requires at least JDK6 for compilation.

The primary entry point to the ShrinkWrap library is the +org.jboss.shrinkwrap.api.ShrinkWrap+ class.  From here you may call the +create+ method to make a new +Archive+, the a generic view of the virtual filesystem which allows the addition of content called +Asset+ s into a location called an +ArchivePath+.  The following table more easily shows ShrinkWrap nomenclature next to more common terms:

|=============================
|_Archive Type_|_Description_
|+org.jboss.shrinkwrap.api.GenericArchive+|Simplest type of concrete user-view of an +Archive+; supports generic operations
|+org.jboss.shrinkwrap.api.spec.JavaArchive+|JAR type; allows addition of +Class+ es, +Package+ s, and Manifest operations
|+org.jboss.shrinkwrap.api.spec.EnterpriseArchive+|Java EE EAR type; supports Manifest and related spec operations
|+org.jboss.shrinkwrap.api.spec.WebArchive+|Java EE WAR type; supports operations common to web application deployments
|+org.jboss.shrinkwrap.api.spec.ResourceAdaptorArchive+|Java EE RAR type; supports operations common to resource adaptor deployments
|=============================

To create an +Archive+, simply choose your desired archive type and optionally supply a name to the static +ShrinkWrap:create+ method:

[source,java]
----
GenericArchive myArchive = ShrinkWrap.create(GenericArchive.class,"myArchive.jar");
----

That's it!  You've got your first ShrinkWrap archive!

Of course, an object representing an empty archive is pretty useless.  So let's have a look at adding in some content.  As we noted before, content is modeled by the +Asset+ class, so let's first take a look at some of the +Asset+ implementations provided by ShrinkWrap:

|=====================
|_Asset_|_Represents_
|+org.jboss.shrinkwrap.api.asset.ArchiveAsset+|Nested +Archive+ content
|+org.jboss.shrinkwrap.api.asset.ByteArrayAsset+|+byte[]+ or +InputStream+ content 
|+org.jboss.shrinkwrap.api.asset.ClassAsset+|Java +Class+ content
|+org.jboss.shrinkwrap.api.asset.ClassLoaderAsset+|A resource which can be loaded by an optionally-specified +ClassLoader+
|+org.jboss.shrinkwrap.api.asset.FileAsset+|+File+ content
|+org.jboss.shrinkwrap.api.asset.StringAsset+|+String+ content
|+org.jboss.shrinkwrap.api.asset.UrlAsset+|Content located at a given +URL+
|+org.jboss.shrinkwrap.api.asset.EmptyAsset+|Empty (0-byte) content
|=====================

Additionally, because +Asset+ is an interface, you may provide your own implementation to supply any byte-based content that may be represented as an +InputStream+ .  For instance, the snippet below shows how to present an Activation Framework +DataSource+ as an +Asset+ :

[source,java]
----
final DataSource dataSource = null; // Assume you have this
Asset asset = new Asset() {
  @Override
  public InputStream openStream() {
    try {
      return dataSource.getInputStream();
    } catch (final IOException e) {
      throw new RuntimeException(e);
    }
  }
};
----

The +Archive:add+ method allows us to pass in some +Asset+ content and add it under an +ArchivePath+.

[source,java]
----
myArchive.add(myAsset,"path/to/content");
System.out.println(myArchive.toString(true));
----

Passing a +true+ verbosity flag into the +toString+ method of +Archive+ creates a recursive +"ls -l"+ -style output:

----
myArchive.jar:
/path/
/path/to/
/path/to/content
----

The +Archive+ views we covered before are also really helpful, depending upon the type of content you're working with.  For instance, a standard JAR file typically contains +.class+ files and other resources, so the +JavaArchive+ type lets you add these.

ShrinkWrap supports a simple mechanism allowing you to switch "views" of your archive, and it's provided by the +as+ method of the +org.jboss.shrinkwrap.api.Assignable+ interface; each view in turn extends +Assignable+. So in order to get your archive to use the +JavaArchive+ view in order to easily add +Class+ resources, you could simply:

[source,java]
----
myArchive.as(JavaArchive.class).addClasses(String.class, Integer.class);
System.out.println(myArchive.toString(true));
----

----
archive.jar:
/java/
/java/lang/
/java/lang/String.class
/java/lang/Integer.class
----

Using this mechanism is central to keeping ShrinkWrap's usage clean and intuitive, while providing for a versatility typically found in true multiple-inheritance languages.

While ShrinkWrap has its roots in Java EE and close ties to the Arquillian Testing Platform, it's certainly not limited to these domains.  In fact, ShrinkWrap on its own intentionally scoped to go no further than act as a virtual filesystem for archives.  As such, it provides a simple mechanism for playing nicely with flat-file structures.

Borrowing from our example above, perhaps we'd like to use ShrinkWrap to package up all of the @.class@ files in the current package and output these as a standard JAR in ZIP format.  The code for that would actually be pretty simple:

[source,java]
----
 JavaArchive archive = ShrinkWrap.create(JavaArchive.class,
  "myPackage.jar").addPackage(this.getClass().getPackage());
  System.out.println(archive.toString(true));
  archive.as(ZipExporter.class).exportTo(
    new File("/home/alr/Desktop/myPackage.jar"), true);
----

----
myPackage.jar:
/org/
/org/alr/
/org/alr/test/
/org/alr/test/TestClass.class
----

So let's see what's going on here.  First we create a +JavaArchive+ and add all contents of the current +Class+ 's +Package+ . Then we dump the output to the console, just to see what's included.  In the final line, we again use the +Assignable+ facilities of the +JavaArchive+ view to get us into a new view: one capable of exporting to ZIP format.  In this case we use the appropriately-named +ZipExporter+, allowing us to export to a +File+, +OutputStream+, or even get the contents as an +InputStream+ so we can deal with the bytes ourselves.

There are 3 types of exporters which ship with ShrinkWrap:

|==========================
|_Exporter_|_Output Format_
|+org.jboss.shrinkwrap.api.exporter.TarExporter+|TAR
|+org.jboss.shrinkwrap.api.exporter.TarGzExporter+|TAR.GZ
|+org.jboss.shrinkwrap.api.exporter.ZipExporter+|ZIP
|==========================

Of course, we can also obtain a ShrinkWrap archive from a flat-file in a similar fashion by using one of the standard importers:

|==========================
|_Importer_|_Output Format_
|+org.jboss.shrinkwrap.api.importer.TarImporter+|TAR
|+org.jboss.shrinkwrap.api.importer.TarGzImporter+|TAR.GZ
|+org.jboss.shrinkwrap.api.importer.ZipImporter+|ZIP
|==========================

The code for running an import to roundtrip the previous example might look like this:

[source,java]
----
 JavaArchive roundtrip = ShrinkWrap
  .create(ZipImporter.class, "myPackageRoundtrip.jar")
  .importFrom(new File("/home/alr/Desktop/myPackage.jar"))
  .as(JavaArchive.class);
----

Note how we can pass +ZipImporter+ into the +ShrinkWrap.create+ method, as it's +Assignable+ as well!  Beginning to notice a theme here?

This concludes our brief introduction into manipulating archive content with ShrinkWrap.

==== ShrinkWrap Resolvers

While ShrinkWrap is ideally-suited to creating new archives containing byte-based resources, often our applications are composed from pre-built libraries into more complex deployments.  These may bundle other archives together, for instance in the following example _Web application ARchive_ (WAR):

----
$> jar -tvf myApplication.war
     0 Tue Apr 23 17:01:08 MST 2013 META-INF/
   128 Tue Apr 23 17:01:06 MST 2013 META-INF/MANIFEST.MF
     0 Tue Apr 23 17:01:08 MST 2013 WEB-INF/
     0 Tue Apr 23 17:01:08 MST 2013 WEB-INF/classes/
     0 Tue Apr 23 17:01:08 MST 2013 WEB-INF/lib/
  3654 Tue Apr 23 16:59:44 MST 2013 WEB-INF/lib/hibernate.jar
  3800 Tue Apr 23 17:01:00 MST 2013 WEB-INF/lib/commons-io.jar
  4015 Tue Apr 23 17:00:44 MST 2013 WEB-INF/lib/myEjbModule.jar
----

As we can see, under +WEB-INF/lib+ there are a couple of thirdparty libraries used as dependencies by our own code, and an _Enterprise JavaBeans_ (EJB) module that we've written for our application.  This packaging structure is consistent with the final deployments used by most WARs and _Enterprise application ARchives_ (EARs).

Often we don't control the construction of these libraries, and we certainly shouldn't be in the business of re-assembling them (and hence further differentiating our tests from the our production runtime deployments).  With the advent of Maven and other build systems, typically thirdparty libraries and our own dependent modules are obtained from a backing software _repository_.  In this case we supply a series of coordinates which uniquely identifies an artifact in the repository, and resolve the target files from there.

That is precisely the aim of the ShrinkWrap Resolvers project; it is a Java API to obtain artifacts from a repository system.  Currently implemented are grammars and support for Maven-based repository structures (this is separate from the use of Maven as a project management system or build tool; it's possible to use a Maven repository layout with other build systems).

ShrinkWrap Resolvers is comprised of the following modules:

|=========================
|_Name_|_Maven Coordinates_
|API|org.jboss.shrinkwrap.resolver:shrinkwrap-resolver-api
|SPI|org.jboss.shrinkwrap.resolver:shrinkwrap-resolver-spi
|Maven API|org.jboss.shrinkwrap.resolver:shrinkwrap-resolver-api-maven
|Maven SPI|org.jboss.shrinkwrap.resolver:shrinkwrap-resolver-spi-maven
|Maven Implementation|org.jboss.shrinkwrap.resolver:shrinkwrap-resolver-impl-maven
|Maven Implementation with Archive Integration|org.jboss.shrinkwrap.resolver:shrinkwrap-resolver-impl-maven-archive
|=========================

The separation between the Maven and non-Maven modules is there to enforce modular design and separate out generic resolution from Maven-specific grammars, should the project support other mechanisms in the future.

Obtaining ShrinkWrap Resolvers for use in your system can be done in a single pass by declaring a dependency upon the +depchain+ module in a Maven +pom.xml+:

[source,xml]
----
<dependencies>
    <dependency>
      <groupId>org.jboss.shrinkwrap.resolver</groupId>
      <artifactId>shrinkwrap-resolver-depchain</artifactId>
      <version>${version.shrinkwrap.resolvers}</version>
    </dependency>
    ...
</dependencies>
----

This will bring the APIs into the compilation classpath and the SPIs and Implementation modules into the runtime classpaths (which will not be transitively inherited, as per Maven rules in +runtime+ scope).

Alternatively, you can have finer-grained control over using ShrinkWrap Resolvers by bringing in each module manually:

[source,xml]
----
 <dependencies>
    <dependency>
      <groupId>org.jboss.shrinkwrap.resolver</groupId>
      <artifactId>shrinkwrap-resolver-api</artifactId>
      <version>${project.version}</version>
    </dependency>
    <dependency>
      <groupId>org.jboss.shrinkwrap.resolver</groupId>
      <artifactId>shrinkwrap-resolver-spi</artifactId>
      <version>${project.version}</version>
      <scope>runtime</scope>
    </dependency>
    <dependency>
      <groupId>org.jboss.shrinkwrap.resolver</groupId>
      <artifactId>shrinkwrap-resolver-api-maven</artifactId>
      <version>${project.version}</version>
    </dependency>
    <dependency>
        <groupId>org.jboss.shrinkwrap.resolver</groupId>
        <artifactId>shrinkwrap-resolver-spi-maven</artifactId>
        <version>${project.version}</version>
        <scope>runtime</scope>
      </dependency>
    <dependency>
      <groupId>org.jboss.shrinkwrap.resolver</groupId>
      <artifactId>shrinkwrap-resolver-impl-maven</artifactId>
      <version>${project.version}</version>
      <scope>runtime</scope>
    </dependency>
    <dependency>
      <groupId>org.jboss.shrinkwrap.resolver</groupId>
      <artifactId>shrinkwrap-resolver-impl-maven-archive</artifactId>
      <version>${project.version}</version>
      <scope>runtime</scope>
    </dependency>
    ...
  </dependencies>
----

The general entry point for resolution is the convenience +org.jboss.shrinkwrap.resolver.api.maven.Maven+ class, which has static hooks to obtain a new +org.jboss.shrinkwrap.resolver.api.maven.MavenResolverSystem+.

[source,java]
----
MavenResolverSystem resolver = Maven.resolver();
----

This is really a shorthand for the typesafe and generic call via the standard +org.jboss.shrinkwrap.resolver.api.Resolvers+ API (which, as mentioned before, is not bound to Maven-specific grammars):

[source,java]
----
MavenResolverSystem resolver = Resolvers.use(MavenResolverSystem.class);
----

We may also want to configure our +MavenResolverSystem+ via some +settings.xml+ file on the filesystem or through the +--classpath+ (ie. visible to the +Thread+ Context +ClassLoader+ or some other explicit +ClassLoader+, and that's easily enough done:

[source,java]
----
MavenResolverSystem resolverConfiguredFromFile = Maven.configureResolver().fromFile("~/.m2/settings.xml");
MavenResolverSystem resolverConfiguredFromClResource = Maven.configureResolver().fromClassloaderResource("settings.xml");
----

This will give us a +MavenResolverSystem+ which should act similar to a Maven build invoked using the +-s [filename]+ switch.

From there, we can begin to carry out all kinds of resolution requests.  Under the hood, the mechanism is powered by the same engine running Maven and the http://www.sonatype.org/m2eclipse/[m2eclipse] IDE Plugin: the http://www.sonatype.org/aether[Aether] project.

Let's say we'd like to resolve a single artifact from its Maven coordinates (_GAV_, or "groupId, artifactId, and version") as a +File+ without any of its dependencies:

[source,java]
----
File artifact = Maven.resolver().resolve("G:A:V").withoutTransitivity()
                .asSingle(File.class);
----

We can also obtain an artifact and all of its dependencies by enabling transitivity:

[source,java]
----
File[] files = Maven.resolver().resolve("G:A:V").withTransitivity().as(File.class);
----

You'll note the +as+ and +asSingle+ methods allow us to specify other formats to receive, so we may prefer to resolve these as a ShrinkWrap +JavaArchive+.

[source,java]
----
JavaArchive artifact = Maven.resolver().resolve("G:A:V").withoutTransitivity()
                .asSingle(JavaArchive.class);
----

Other valid types to these methods include +InputStream+ and +org.jboss.shrinkwrap.resolver.api.maven.MavenResolvedArtifact+.  There are shortcut methods for these as well:

[source,java]
----
File file = Maven.resolver().resolve("G:A:V").withoutTransitivity()
                 // Resolve as a single file; error if more than one result was resolved
                .asSingleFile();
File[] artifactAndDependencies = Maven.resolver().resolve("G:A:V").withTransitivity()
                // Return all resolved artifacts as an array of Files
                .asFile();
----

The +MavenResolvedArtifact+ grants us a peek into some of the resolution data:

[source,java]
----
MavenResolvedArtifact artifact = Maven.resolver().resolve("G:A:V").withoutTransitivity()
                .asSingle(MavenResolvedArtifact.class);
MavenCoordinate coordinate = artifact.getCoordinate();
String groupId = coordinate.getGroupId();
String artifactId = coordinate.getArtifactId();
String version = coordinate.getVersion();
String resolvedVersion = artifact.getResolvedVersion();
String type = coordinate.getType().toString();
boolean isSnapshot = artifact.isSnapshotVersion();
String classifier = coordinate.getClassifier();
File file = artifact.asFile();
File file2 = artifact.as(File.class);
InputStream in = artifact.as(InputStream.class);
InputStream in2 = artifact.as(InputStream.class);
JavaArchive archive = artifact.as(JavaArchive.class);
----

Perhaps we'd like to work only from the backing local repositories on disk, skipping any defined remote repositories (including Maven Central); this is done by setting +offline+ on the +MavenResolverSystem:

[source,java]
----
Maven.resolver().offline(true).resolve("groupId:artifactId:version").withoutTransitivity().asSingle(File.class);
----

We can also have finer-grained control over the repositories used.  For instance, we may wish to disable resolution from the +--classpath+ (which is a default):

[source,java]
----
Maven.resolver().resolve("G:A:V").withClassPathResolution(false)
            .withTransitivity().asFile();
----

Or maybe we'd like to not consult the Maven Central Repository during resolution:

[source,java]
----
Maven.resolver().resolve("G:A:V").withMavenCentralRepo(false).
            withTransitivity().asFile();
----

Often, we have a Maven _POM_ (Project Object Model) file which contains dependency information about our current project.  In many cases it might be more desirable to pull information from there rather than hardcode and potentially duplicate dependency information in our code.  Therefore ShrinkWrap Resolvers supports the "loading" of POM metadata from a +File+ or +ClassLoader+ resource:

[source,java]
----
// Load from File
Maven.resolver().loadPomFromFile("path/to/pom").resolve("G:A").withoutTransitivity()
                .asFile();
// Load From CL resource
Maven.resolver().loadPomFromClassloaderResource("pom.xml").resolve("G:A").withoutTransitivity()
                .asFile();
----

In the above code, we can omit the "V" portion of the _GAV_ coordinates, the version, because that information is defined in the +dependencies+ section of the loaded POM.

Or perhaps we'd like to obtain all dependencies defined in the POM:

[source,java]
----
File[] allDependencies = Maven.resolver().loadPomFromFile("path/to/pom").importRuntimeAndTestDependencies().asFile();
----

We can also obtain the runtime dependencies only:

[source,java]
----
File[] runtimeDependencies = Maven.resolver().loadPomFromFile("path/to/pom").importRuntimeDependencies().asFile();
----

The +loadPomFromFile+ and +loadPomFromClassloaderResource+ methods both have overloaded variants to accept one or more Maven profiles to enable during execution, similar to providing the +-P [profileName]+ option on the command line:

[source,java]
----
Maven.resolver().loadPomFromFile("path/to/pom", "profileName").
		importRuntimeDependencies().asFile();
----

By enabling resolution in a friendly, intuitive API, ShrinkWrap Resolvers arms ShrinkWrap archives with a powerful mechanism to create deployment units which are applicable in real-world scenarios that demand libraries and modules not owned by the current project.

=== Runtime

Being simply a component model, Java EE needs a concrete implementation to provide the runtime services to our applications.

==== JBoss Application Server 7

image:images/ch02-enabling_technologies/jbossas7.png["JBoss AS7"]

The latest community edition of the http://www.jboss.org/jbossas[JBoss Application Server 7] will be the target runtime for our examples.  Written from the ground up, AS7 was designed with the following goals at the core:

* *Speed*.  Startup, deployment, and request processing demands leverage a concurrent state machine and constant-time ClassLoading.
* *Efficiency*.  Memory usage is kept to a minimum.
* *Modularity*.  Application libraries and server libraries are isolated from one another to avoid runtime conflicts
* *Administration*.  Centralized settings via Web Interface, HTTP, Java, and Command-Line APIs
* *Compliance*.  http://fedoraproject.org/wiki/Features/JBossAS7#Strict_Compliance[Java EE6 Full Profile Certification]
* *Testable*.  Uses Arquillian and ShrinkWrap in its own internal test suite

Because a quick feedback loop is important in testing during development, the speed afforded by JBossAS7 makes it a compelling candidate for our target runtime:

----
08:23:26,759 INFO  [org.jboss.as] (Controller Boot Thread) JBAS015874: JBoss AS 7.1.1.Final "Brontes" started in 1775ms - Started 133 of 208 services (74 services are passive or on-demand)
----

==== OpenShift

image:images/ch02-enabling_technologies/openshift.png["OpenShift"]

While getting our applications running on our own machine is a great step in developing, the beauty of the internet is that we can expose our content and services to the world at large.  Until very recently, Java EE hosting typically involved a dedicated and expensive server colocated in a data center.  With the rapid advent of virtualization and the Cloud, we're now able to gain public access much more easily, and at far reduced cost.

OpenShift is Red Hat's free Platform as a Service (PaaS) for applications.  While it supports a variety of frameworks bundled as "cartridges", we'll be using OpenShift's built-in JBossAS7 support.  With just a little bit of initial setup, pushing changes from our local Git repository to the OpenShift remote will trigger a build and deployment of our application for all to see.  We'll be relieved of the responsibility to obtain a server, install JBossAS, configure the networking and firewalls, or manually deploy new versions.

=== On to the Code

Now that we've familiarized ourselves with the technologies we'll be using throughout the exercises, let's dig in and create a new Java EE application, making it public to the world.
